---
title: "My Document"
author: "Me"
date: "Today"
output:
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{fontspec}
  - \usepackage{xeCJK}
---
```{r include=FALSE}
library(tidyverse)
library(ggplot2)
library(mlbench)
library(caret)
library(glmnet)
library(mlbench)
library(pROC)
library(pdp)
library(vip)
library(AppliedPredictiveModeling)
library(MASS)
library(rpart.plot)
library(ggpubr)

```

# Import data
```{r}
l<-load("recovery.Rdata")

dat <- eval(parse(text = l))%>%
  mutate(study = factor(study))

head(dat)

set.seed(5296)
data.1 <- dat[sample(1:10000, 2000),]
```

```{r}
set.seed(5095)
data.2 <- dat[sample(1:10000, 2000),] 
```

```{r}

reco.data<-rbind(data.1, data.2)%>%
  unique.array()%>%
  dplyr::select(-id)

head(reco.data)
```


```{r}

reco.data.bin<-reco.data%>%
  mutate(recovery_time = factor(ifelse(recovery_time>30,"long","short")))
```
## Split data
```{r}
set.seed(2023)
rowtrain <- createDataPartition(y=reco.data.bin$recovery_time, p=0.8, list=FALSE)
training_set.bin <- reco.data.bin[rowtrain,]
test_set.bin <- reco.data.bin[-rowtrain,]
x_train.bin <- model.matrix(recovery_time~.,training_set.bin)[,-1]
y_train.bin <- factor(training_set.bin$recovery_time)
x_test.bin <- model.matrix(recovery_time~.,test_set.bin)[,-1]
y_test.bin <- factor(test_set.bin$recovery_time)
contrasts(y_train.bin)
```

# EDA
```{r}
table(y_train.bin)
ggplot(training_set.bin, aes(y_train.bin)) + 
  geom_bar()+
  ggtitle("Recovery")
```

```{r}
p1 <- ggplot(training_set.bin,aes(x = recovery_time, y = age)) +
  geom_boxplot()
p2 <- ggplot(training_set.bin,aes(x = recovery_time, y = bmi)) +
  geom_boxplot()
p5 <- ggplot(training_set.bin,aes(x = recovery_time, y = height)) +
  geom_boxplot()
p6 <- ggplot(training_set.bin,aes(x = recovery_time, y = weight)) +
  geom_boxplot()
p3 <- ggplot(training_set.bin,aes(x = recovery_time, y = SBP)) +
  geom_boxplot()
p4 <- ggplot(training_set.bin,aes(x = recovery_time, y = LDL)) +
  geom_boxplot()

arrange = ggarrange(p1,p2,p5,p6, p3,p4, ncol = 3, nrow = 2)
ggsave("arrangedplot3.png", arrange)
```

# Modeling

## logistic Regression

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     summaryFunction = twoClassSummary,
                      classProbs = TRUE)

set.seed(2023)

logit.fit <- train(x= x_train.bin, 
                  y= y_train.bin, 
                  method = 'glm',
                  metric = "ROC",
                  family = binomial(),
                  trControl = ctrl )
```

```{r}
# Performance
coef(logit.fit$finalModel)%>%knitr::kable()


# ROC Curve

pred.logit.1 <- predict(logit.fit, newdata  = x_test.bin, type = "prob")[,2]
roc.logit <- roc(y_test.bin, pred.logit.1)
plot(roc.logit, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.logit), col = 4, add = TRUE)

# Confusion Matrix
logit.pred.2 <- predict(logit.fit, newdata  = x_test.bin)
confusionMatrix(data = logit.pred.2   ,
                 y_test.bin)

```
The accuracy of logistic regression with the best tunning parameter is 0.6885


## MARS
```{r}
set.seed(2023)
mars.fit <- train(x = x_train.bin,
                   y = y_train.bin,
                   method = "earth",
                    tuneGrid = expand.grid(degree = 1:3,
                    nprune = 2:19),
                    metric = "ROC",
                    trControl = ctrl)
```

```{r}
#performance
plot(mars.fit)
mars.fit$bestTune
coef(mars.fit$finalModel)%>%knitr::kable()
# ROC Curve
pred.mars.1 <- predict(mars.fit, newdata  = x_test.bin, type = "prob")[,2]
mars.roc <- roc(y_test.bin, pred.mars.1)
plot(mars.roc, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(mars.roc), col = 4, add = TRUE)
# Confusion Matrix
pred.mars.2 <- predict(mars.fit, newdata  = x_test.bin)
confusionMatrix(data = pred.mars.2,
                reference = y_test.bin)
p1 <- pdp::partial(mars.fit, pred.var = c("bmi"), grid.resolution = 10) %>% autoplot()
p2 <- pdp::partial(mars.fit, pred.var = c("SBP"), grid.resolution = 10) %>% autoplot()
p4 <- pdp::partial(mars.fit, pred.var = c("bmi", "SBP"), 
                   grid.resolution = 10) %>%
      pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
                       screen = list(z = 20, x = -60))
p1
p2
p4

vip(mars.fit$finalModel)
```

## LDA
```{r}
#LDA
lda.fit <- train(x = x_train.bin,
                   y = y_train.bin,
                   method = "lda",
                   metric = "ROC",
                   trControl = ctrl)
plot(lda(y_train.bin~x_train.bin))
```

```{r}

#Performance
coef(lda.fit$finalModel)


# ROC Curve
pred.lda.1 <- predict(lda.fit, newdata  = x_test.bin, type = "prob")[,2]
lda.roc <- roc(y_test.bin, pred.lda.1)
plot(lda.roc, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(lda.roc), col = 4, add = TRUE)

# Confusion Matrix
lda.pred.2 <- predict(lda.fit, newdata  = x_test.bin)
confusionMatrix(data = lda.pred.2,
                reference = y_test.bin)

```
## Classification Tree
```{r}

set.seed(2023)
rpart.fit <- train(x=x_train.bin, y = y_train.bin,
                       method = "rpart",
                       tuneGrid = data.frame(cp = exp(seq(-8,-5, len = 50))),
                       trControl = ctrl,
                       metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)


```

```{r}
rpart.fit$bestTune
rpart.plot(rpart.fit$finalModel)
```

```{r}

# Performance

# ROC Curve
pred.rpart.1 <- predict(rpart.fit, newdata  = x_test.bin, type = "prob")[,2]
rpart.roc <- roc(y_test.bin, pred.rpart.1)
plot(rpart.roc, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(rpart.roc), col = 4, add = TRUE)

# Confusion Matrix
rpart.pred.2 <- predict(rpart.fit, newdata = x_test.bin)
confusionMatrix(data = rpart.pred.2,
                reference = y_test.bin)

```



## Model Comparison
```{r}
set.seed(2023)
res <- caret::resamples(list(Logit = logit.fit, Mars = mars.fit, lda = lda.fit, rpart=rpart.fit),metric = c("F", "ROC", "Accuracy", "Precision"),
                 method = "cv",index = createFolds(trainData$binary_outcome, k = 10))

summary(res)
```






